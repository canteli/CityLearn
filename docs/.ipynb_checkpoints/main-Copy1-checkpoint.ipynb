{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from citylearn import  CityLearn, building_loader, auto_size\n",
    "from energy_models import HeatPump, EnergyStorage, Building\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "import gym\n",
    "from gym.utils import seeding\n",
    "from gym import core, spaces\n",
    "import os\n",
    "import ptan\n",
    "import time\n",
    "import argparse\n",
    "import model, common\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np\n",
    "from reward_function import reward_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_folder = Path(\"data/\")\n",
    "\n",
    "demand_file = data_folder / \"AustinResidential_TH.csv\"\n",
    "weather_file = data_folder / 'Austin_Airp_TX-hour.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_ids = [4, 5, 9, 16, 21, 26, 33, 36, 49, 59]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_pump, heat_tank, cooling_tank = {}, {}, {}\n",
    "\n",
    "#Ref: Assessment of energy efficiency in electric storage water heaters (2008 Energy and Buildings)\n",
    "loss_factor = 0.19/24\n",
    "buildings = []\n",
    "for uid in building_ids:\n",
    "    heat_pump[uid] = HeatPump(nominal_power = 9e12, eta_tech = 0.22, t_target_heating = 45, t_target_cooling = 10)\n",
    "    heat_tank[uid] = EnergyStorage(capacity = 9e12, loss_coeff = loss_factor)\n",
    "    cooling_tank[uid] = EnergyStorage(capacity = 9e12, loss_coeff = loss_factor)\n",
    "    buildings.append(Building(uid, heating_storage = heat_tank[uid], cooling_storage = cooling_tank[uid], heating_device = heat_pump[uid], cooling_device = heat_pump[uid]))\n",
    "    buildings[-1].state_space(np.array([24.0, 40.0, 1.001]), np.array([1.0, 17.0, -0.001]))\n",
    "    buildings[-1].action_space(np.array([0.5]), np.array([-0.5]))\n",
    "    \n",
    "building_loader(demand_file, weather_file, buildings)  \n",
    "auto_size(buildings, t_target_heating = 45, t_target_cooling = 10)\n",
    "env = CityLearn(demand_file, weather_file, buildings = buildings, time_resolution = 1, simulation_period = (3500,6000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGActor(nn.Module):\n",
    "    def __init__(self, obs_size, act_size):\n",
    "        super(DDPGActor, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, act_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DDPGCritic(nn.Module):\n",
    "    def __init__(self, obs_size, act_size):\n",
    "        super(DDPGCritic, self).__init__()\n",
    "\n",
    "        self.obs_net = nn.Sequential(\n",
    "            nn.Linear(obs_size, 8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.out_net = nn.Sequential(\n",
    "            nn.Linear(8 + act_size, 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        obs = self.obs_net(x)\n",
    "        return self.out_net(torch.cat([obs, a], dim=1))\n",
    "    \n",
    "class TargetNet:\n",
    "    \"\"\"\n",
    "    Wrapper around model which provides copy of it instead of trained weights\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.target_model = copy.deepcopy(model)\n",
    "\n",
    "    def sync(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def alpha_sync(self, alpha):\n",
    "        \"\"\"\n",
    "        Blend params of target net with params from the model\n",
    "        :param alpha:\n",
    "        \"\"\"\n",
    "        assert isinstance(alpha, float)\n",
    "        assert 0.0 < alpha <= 1.0\n",
    "        state = self.model.state_dict()\n",
    "        tgt_state = self.target_model.state_dict()\n",
    "        for k, v in state.items():\n",
    "            tgt_state[k] = tgt_state[k] * alpha + (1 - alpha) * v\n",
    "        self.target_model.load_state_dict(tgt_state)\n",
    "        \n",
    "    \n",
    "class Batch:\n",
    "    def __init__(self):\n",
    "        self.batch = []\n",
    "        \n",
    "    def append_sample(self, sample):\n",
    "        self.batch.append(sample)\n",
    "        \n",
    "    def sample(self, sample_size):\n",
    "        s, a, r, s_next = [],[],[],[]\n",
    "        rand_sample = random.sample(self.batch[i], BATCH_SIZE)\n",
    "        for values in rand_sample:\n",
    "            s.append(values[0])\n",
    "            a.append(values[1])\n",
    "            r.append(values[2])\n",
    "            s_next.append(values[3])\n",
    "        return s, a, r, s_next\n",
    "    \n",
    "    def length(self):\n",
    "         return len(self.batch)\n",
    "        \n",
    "    \n",
    "class RL_Agents:\n",
    "    def __init__(self, observation_spaces = None, action_spaces = None):\n",
    "        self.device = \"cpu\"\n",
    "        self.epsilon = 0.3\n",
    "        self.n_buildings = len(observation_spaces)\n",
    "        self.batch = {}\n",
    "        for i in range(len(observation_spaces)):\n",
    "            self.batch[i] = Batch()\n",
    "            \n",
    "        LEARNING_RATE_ACTOR = 1e-4\n",
    "        LEARNING_RATE_CRITIC = 1e-3\n",
    "        MIN_REPLAY_MEMORY = 100\n",
    "        BATCH_SIZE = 50000\n",
    "        EPOCHS = 6\n",
    "        \n",
    "        i = 0\n",
    "        self.act_net, self.crt_net, self.tgt_act_net, self.tgt_crt_net, self.act_opt, self.crt_opt = {}, {}, {}, {}, {}, {}\n",
    "        for o, a in zip(observation_spaces, action_spaces):\n",
    "            self.act_net[i] = DDPGActor(o.shape[0], a.shape[0]).to(self.device)\n",
    "            self.crt_net[i] = DDPGCritic(o.shape[0], a.shape[0]).to(self.device)\n",
    "            self.tgt_act_net[i] = TargetNet(self.act_net[i])\n",
    "            self.tgt_crt_net[i] = TargetNet(self.crt_net[i])\n",
    "            self.act_opt[i] = optim.Adam(self.act_net[i].parameters(), lr=LEARNING_RATE_ACTOR)\n",
    "            self.crt_opt[i] = optim.Adam(self.crt_net[i].parameters(), lr=LEARNING_RATE_CRITIC)\n",
    "            i += 1\n",
    "        \n",
    "    def select_action(self, states):\n",
    "        i, actions = 0, []\n",
    "        for state in states:\n",
    "            a = self.act_net[i](torch.tensor(state))\n",
    "            print(a)\n",
    "            a = (1+self.epsilon) * np.random.normal(size=a.shape)\n",
    "            print(a)\n",
    "            a = np.clip(a, -1, 1)\n",
    "            actions.append(a)\n",
    "            i += 1\n",
    "        return actions\n",
    "    \n",
    "    def add_to_batch(self, states, actions, rewards, next_states):\n",
    "        i = 0\n",
    "        for s, a, r, s_next in zip(states, actions, rewards, next_states):\n",
    "            self.batch[i].append_sample((s, a, r, s_next))\n",
    "            i += 1\n",
    "            \n",
    "        batch, states_v, actions_v, rewards_v, dones_mask, last_states_v, q_v, last_act_v, q_last_v, q_ref_v, critic_loss_v, cur_actions_v, actor_loss_v = {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}  \n",
    "        if self.batch[i].length() > MIN_REPLAY_MEMORY:\n",
    "            for i in range(self.n_buildings):\n",
    "                for k in range(EPOCHS):\n",
    "                    states_v[i], actions_v[i], rewards_v[i], states_next_v[i] = self.batch[i].sample(BATCH_SIZE)\n",
    "\n",
    "                    # TRAIN CRITIC\n",
    "                    crt_opt[i].zero_grad()\n",
    "                    #Obtaining Q' using critic net with parameters teta_Q'\n",
    "                    q_v[i] = self.crt_net[i](states_v[i], actions_v[i])\n",
    "\n",
    "                    #Obtaining estimated optimal actions a|teta_mu from target actor net and from s_i+1.\n",
    "                    last_act_v[i] = self.tgt_act_net[i].target_model(last_states_v[i]) #<----- Actor to train Critic\n",
    "\n",
    "                    #Obtaining Q'(s_i+1, a|teta_mu) from critic net Q'\n",
    "                    q_last_v[i] = self.tgt_crt_net[i].target_model(last_states_v[i], last_act_v[i])\n",
    "                    q_last_v[i][dones_mask[i]] = 0.0\n",
    "\n",
    "                    #Q_target used to train critic net Q'\n",
    "                    q_ref_v[i] = rewards_v[i].unsqueeze(dim=-1) + q_last_v[i] * GAMMA\n",
    "                    critic_loss_v[i] = F.mse_loss(q_v[i], q_ref_v[i].detach())\n",
    "                    critic_loss_v[i].backward()\n",
    "                    crt_opt[i].step()\n",
    "\n",
    "                    # TRAIN ACTOR\n",
    "                    act_opt[i].zero_grad()\n",
    "                    #Obtaining estimated optimal current actions a|teta_mu from actor net and from s_i\n",
    "                    cur_actions_v[i] = self.act_net[i](states_v[i])\n",
    "\n",
    "                    #Actor loss = mean{ -Q_i'(s_i, a|teta_mu) }\n",
    "                    actor_loss_v[i] = -self.crt_net[i](states_v[i], cur_actions_v[i]) #<----- Critic to train Actor\n",
    "                    actor_loss_v[i] = actor_loss_v[i].mean()\n",
    "                    #Find gradient of the loss and backpropagate to perform the updates of teta_mu\n",
    "                    actor_loss_v[i].backward()\n",
    "                    act_opt[i].step()\n",
    "\n",
    "                    \n",
    "                    self.tgt_act_net[i].alpha_sync(alpha=1 - 0.1)\n",
    "                    self.tgt_crt_net[i].alpha_sync(alpha=1 - 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import RL_Agents\n",
    "from reward_function import reward_function\n",
    "agent = RL_Agents()\n",
    "episodes = 1\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agents.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = reward_function(rewards)\n",
    "        agents.add_to_batch(state, action, reward, next_state)\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGActor(nn.Module):\n",
    "    def __init__(self, obs_size, act_size):\n",
    "        super(DDPGActor, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, act_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DDPGCritic(nn.Module):\n",
    "    def __init__(self, obs_size, act_size):\n",
    "        super(DDPGCritic, self).__init__()\n",
    "\n",
    "        self.obs_net = nn.Sequential(\n",
    "            nn.Linear(obs_size, 8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.out_net = nn.Sequential(\n",
    "            nn.Linear(8 + act_size, 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        obs = self.obs_net(x)\n",
    "        return self.out_net(torch.cat([obs, a], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b, c, d in zip([1,2,3],[1,2,3],[1,2,3],[1,2,3]):\n",
    "    print(a,b,c,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[i] for i in range(1,11)]\n",
    "a, r, terminal, _ = env.step(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.sample([(1,2),(3,4),(5,6),(7,8)],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_function(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    N_AGENTS = 2\n",
    "    GAMMA = 0.99\n",
    "    BATCH_SIZE = 5000\n",
    "    LEARNING_RATE_ACTOR = 1e-4\n",
    "    LEARNING_RATE_CRITIC = 1e-3\n",
    "    REPLAY_SIZE = 5000\n",
    "    REPLAY_INITIAL = 100\n",
    "    TEST_ITERS = 120\n",
    "    EPSILON_DECAY_LAST_FRAME = 1000\n",
    "    EPSILON_START = 1.2\n",
    "    EPSILON_FINAL = 0.02\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    act_net, crt_net, tgt_act_net, tgt_crt_net, agent, exp_source, buffer, act_opt, crt_opt, frame_idx = {}, {}, {}, {}, {}, {}, {}, {}, {}, {}\n",
    "    rew_last_1000, rew, track_loss_critic, track_loss_actor = {}, {}, {}, {}\n",
    "#     for uid in buildings:\n",
    "#         env[uid].reset()\n",
    "    for uid in building_ids:\n",
    "        #Create as many actor and critic nets as number of agents\n",
    "        #Actor: states_agent_i -> actions_agent_i\n",
    "        act_net[uid] = DDPGActor(buildings[uid].observation_spaces.shape[0], buildings[uid].action_spaces.shape[0]).to(device)\n",
    "\n",
    "        #Critic: states_all_agents + actions_all_agents -> Q-value_agent_i [1]\n",
    "        crt_net[uid] = DDPGCritic(buildings[uid].observation_spaces.shape[0], buildings[uid].action_spaces.shape[0]).to(device)\n",
    "\n",
    "        tgt_act_net[uid] = ptan.agent.TargetNet(act_net[uid])\n",
    "        tgt_crt_net[uid] = ptan.agent.TargetNet(crt_net[uid])\n",
    "\n",
    "        agent[uid] = model.AgentD4PG(act_net[uid], device=device)\n",
    "        exp_source[uid] = ptan.experience.ExperienceSourceFirstLast(env[uid], agent[uid], gamma=GAMMA, steps_count=1)\n",
    "        buffer[uid] = ptan.experience.ExperienceReplayBuffer(exp_source[uid], buffer_size=REPLAY_SIZE)\n",
    "        act_opt[uid] = optim.Adam(act_net[uid].parameters(), lr=LEARNING_RATE_ACTOR)\n",
    "        crt_opt[uid] = optim.Adam(crt_net[uid].parameters(), lr=LEARNING_RATE_CRITIC)\n",
    "\n",
    "        frame_idx[uid] = 0\n",
    "\n",
    "        rew_last_1000[uid], rew[uid], track_loss_critic[uid], track_loss_actor[uid] = [], [], [], []\n",
    "\n",
    "    batch, states_v, actions_v, rewards_v, dones_mask, last_states_v, q_v, last_act_v, q_last_v, q_ref_v, critic_loss_v, cur_actions_v, actor_loss_v = {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}\n",
    "    cost, price_list, buffer_reward = {},{},{}\n",
    "    for uid in buildings:\n",
    "        cost[uid] = []\n",
    "        price_list[uid] = []\n",
    "        buffer_reward[uid] = []\n",
    "    while not env[building_ids[-1]]._terminal():\n",
    "        if frame_idx[4]%100 == 0:\n",
    "            print(frame_idx[uid])\n",
    "        for uid in buildings:\n",
    "#             print(env[uid].time_step)\n",
    "            agent[uid].epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx[uid] / EPSILON_DECAY_LAST_FRAME)\n",
    "            frame_idx[uid] += 1           \n",
    "            buffer[uid].populate(1)\n",
    "#             print(buffer[uid].buffer[-1])\n",
    "#             print(env[uid].buildings[uid].time_step)\n",
    "            \n",
    "        \n",
    "        price = env[uid].total_electric_consumption[-1]*3e-5 + 0.045\n",
    "        price_list[uid].append(price)\n",
    "        for uid in buildings:  \n",
    "            buffer_reward[uid].append(buffer[uid].buffer[-1].reward)\n",
    "            electricity_cost = buffer[uid].buffer[-1].reward*price\n",
    "            cost[uid].append(-electricity_cost)\n",
    "            buffer[uid].buffer[-1] = buffer[uid].buffer[-1]._replace(reward=electricity_cost)\n",
    "\n",
    "        if len(buffer[uid]) < REPLAY_INITIAL:\n",
    "            continue   \n",
    "\n",
    "        for uid in buildings:\n",
    "            for k in range(6):\n",
    "                batch[uid] = buffer[uid].sample(BATCH_SIZE)\n",
    "                states_v[uid], actions_v[uid], rewards_v[uid], dones_mask[uid], last_states_v[uid] = common.unpack_batch_ddqn(batch[uid], device) \n",
    "\n",
    "                # TRAIN CRITIC\n",
    "                crt_opt[uid].zero_grad()\n",
    "                #Obtaining Q' using critic net with parameters teta_Q'\n",
    "                q_v[uid] = crt_net[uid](states_v[uid], actions_v[uid])\n",
    "\n",
    "                #Obtaining estimated optimal actions a|teta_mu from target actor net and from s_i+1.\n",
    "                last_act_v[uid] = tgt_act_net[uid].target_model(last_states_v[uid]) #<----- Actor to train Critic\n",
    "\n",
    "                #Obtaining Q'(s_i+1, a|teta_mu) from critic net Q'\n",
    "                q_last_v[uid] = tgt_crt_net[uid].target_model(last_states_v[uid], last_act_v[uid])\n",
    "                q_last_v[uid][dones_mask[uid]] = 0.0\n",
    "\n",
    "                #Q_target used to train critic net Q'\n",
    "                q_ref_v[uid] = rewards_v[uid].unsqueeze(dim=-1) + q_last_v[uid] * GAMMA\n",
    "                critic_loss_v[uid] = F.mse_loss(q_v[uid], q_ref_v[uid].detach())\n",
    "                critic_loss_v[uid].backward()\n",
    "                crt_opt[uid].step()\n",
    "\n",
    "                # TRAIN ACTOR\n",
    "                act_opt[uid].zero_grad()\n",
    "                #Obtaining estimated optimal current actions a|teta_mu from actor net and from s_i\n",
    "                cur_actions_v[uid] = act_net[uid](states_v[uid])\n",
    "\n",
    "                #Actor loss = mean{ -Q_i'(s_i, a|teta_mu) }\n",
    "                actor_loss_v[uid] = -crt_net[uid](states_v[uid], cur_actions_v[uid]) #<----- Critic to train Actor\n",
    "                actor_loss_v[uid] = actor_loss_v[uid].mean()\n",
    "                #Find gradient of the loss and backpropagate to perform the updates of teta_mu\n",
    "                actor_loss_v[uid].backward()\n",
    "                act_opt[uid].step()\n",
    "\n",
    "                if frame_idx[uid] % 1 == 0:\n",
    "                    tgt_act_net[uid].alpha_sync(alpha=1 - 0.1)\n",
    "                    tgt_crt_net[uid].alpha_sync(alpha=1 - 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting all the individual actions\n",
    "figure(figsize=(18, 6))\n",
    "for uid in buildings:\n",
    "    plt.plot(env[uid].action_track[uid][2300:2500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "from networks.actor import Actor\n",
    "from networks.critic import Critic\n",
    "\n",
    "from collections import deque\n",
    "import tensorflow\n",
    "import random\n",
    "\n",
    "\n",
    "class DDPG(object):\n",
    "    def __init__(self, state_size, action_size, actor_hidden_units=(300, 600),\n",
    "                 actor_learning_rate=0.0001, critic_hidden_units=(300, 600),\n",
    "                 critic_learning_rate=0.001, batch_size=64, discount=0.99,\n",
    "                 memory_size=10000, tau=0.001):\n",
    "        \"\"\"\n",
    "        Constructs a DDPG Agent with the given parameters\n",
    "        :param state_size: Int denoting the world's state dimensionality\n",
    "        :param action_size: Int denoting the world's action dimensionality\n",
    "        :param actor_hidden_units: Tuple(Int) denoting the actor's hidden layer\n",
    "            sizes. Each element in the tuple represents a layer in the Actor\n",
    "            network and the Int denotes the number of neurons in the layer.\n",
    "        :param actor_learning_rate: Float denoting the learning rate of the\n",
    "            Actor network. Best to be some small number close to 0.\n",
    "        :param critic_hidden_units: Tuple(Int) denoting the critic's hidden\n",
    "            layer sizes. Each element in the tuple represents a layer in the\n",
    "            Critic network and the Int denotes the number of neurons in the\n",
    "            layer.\n",
    "        :param critic_learning_rate: Float denoting the learning rate of the\n",
    "            Critic network. Best to be some small number close to 0.\n",
    "        :param batch_size: Int denoting the batch size for training.\n",
    "        :param discount: Float denoting the discount (gamma) given to future\n",
    "            potentioal rewards when calculating q values\n",
    "        :param memory_size: Int denoting the number of State, action, rewards\n",
    "            that the agent will remember\n",
    "        :param tau:\n",
    "        \"\"\"\n",
    "\n",
    "        self._discount = discount\n",
    "        self._batch_size = batch_size\n",
    "        self._memory_size = memory_size\n",
    "\n",
    "        tensorflow_session = self._generate_tensorflow_session\n",
    "\n",
    "        self._actor = Actor(tensorflow_session=tensorflow_session,\n",
    "                            state_size=state_size, action_size=action_size,\n",
    "                            hidden_units=actor_hidden_units,\n",
    "                            learning_rate=actor_learning_rate,\n",
    "                            batch_size=batch_size, tau=tau)\n",
    "\n",
    "        self._critic = Critic(tensorflow_session=tensorflow_session,\n",
    "                              state_size=state_size, action_size=action_size,\n",
    "                              hidden_units=critic_hidden_units,\n",
    "                              learning_rate=critic_learning_rate,\n",
    "                              batch_size=batch_size, tau=tau)\n",
    "\n",
    "        self._memory = deque()\n",
    "\n",
    "    def _generate_tensorflow_session(self):\n",
    "        \"\"\"\n",
    "        Generates and returns the tensorflow session\n",
    "        :return: the Tensorflow Session\n",
    "        \"\"\"\n",
    "        config = tensorflow.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        return tensorflow.Session(config=config)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Returns the best action predicted by the agent given the current state.\n",
    "        :param state: numpy array denoting the current state.\n",
    "        :return: numpy array denoting the predicted action.\n",
    "        \"\"\"\n",
    "        return self._actor._model.predict(state)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the DDPG Agent from it's current memory\n",
    "        Please note that the agent must have gone through more steps than the\n",
    "        specified batch size before this method will do anything\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        if len(self._memory) > self._batch_size:\n",
    "            self._train()\n",
    "\n",
    "    def _train(self):\n",
    "        \"\"\"\n",
    "        Helper method for train. Takes care of sampling, and training and\n",
    "        updating both the actor and critic networks\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        states, actions, rewards, done, next_states = self._get_sample()\n",
    "        self._train_critic(states, actions, next_states, done, rewards)\n",
    "        self._train_actor(states)\n",
    "        self._update_target_models()\n",
    "\n",
    "    def _get_sample(self):\n",
    "        \"\"\"\n",
    "        Finds a random sample of size self._batch_size from the agent's current\n",
    "        memory.\n",
    "        :return: Tuple(List(Float, Boolean))) denoting the sample of states,\n",
    "            actions, rewards, done, and next states.\n",
    "        \"\"\"\n",
    "        sample = random.sample(self._memory, self._batch_size)\n",
    "        states, actions, rewards, done, next_states = zip(*sample)\n",
    "        return states, actions, rewards, done, next_states\n",
    "\n",
    "    def _train_critic(self, states, actions, next_states, done, rewards):\n",
    "        \"\"\"\n",
    "        Trains the critic network\n",
    "        C(s, a) -> q\n",
    "        :param states: List of the states to train the network with\n",
    "        :param actions: List of the actions to train the network with\n",
    "        :param next_states: List of the t+1 states to train the network with\n",
    "        :param rewards: List of rewards to calculate q_targets.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        q_targets = self._get_q_targets(next_states, done, rewards)\n",
    "        self._critic.train(states, actions, q_targets)\n",
    "\n",
    "    def _get_q_targets(self, next_states, done, rewards):\n",
    "        \"\"\"\n",
    "        Calculates the q targets with the following formula\n",
    "        q = r + gamma * next_q\n",
    "        unless there is no next state in which\n",
    "        q = r\n",
    "        :param next_states: List(List(Float)) Denoting the t+1 state\n",
    "        :param done: List(Bool) denoting whether each step was an exit step\n",
    "        :param rewards: List(Float) Denoting the reward given in each step\n",
    "        :return: The q targets\n",
    "        \"\"\"\n",
    "        next_actions = self._actor._model.predict(next_states)\n",
    "        next_q_values = self._critic._target_model.predict(next_states,\n",
    "                                                           next_actions)\n",
    "        q_targets = [reward if this_done else reward + self._discount *\n",
    "                                                       (next_q_value)\n",
    "                     for (reward, next_q_value, this_done)\n",
    "                     in zip(rewards, next_q_values, done)]\n",
    "        return q_targets\n",
    "\n",
    "    def _train_actor(self, states):\n",
    "        \"\"\"\n",
    "        Trains the actor network using the calculated deterministic policy\n",
    "            gradients.\n",
    "        :param states: List(List(Float)) denoting he states to train the Actor\n",
    "            on\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        gradients = self._get_gradients(states)\n",
    "        self._actor.train(states, gradients)\n",
    "\n",
    "    def _get_gradients(self, states):\n",
    "        \"\"\"\n",
    "        Calculates the Deterministic Policy Gradient for Actor training\n",
    "        :param states: The states to calculate the gradients for.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        action_for_gradients = self._actor._model.predict(states)\n",
    "        self._critic.get_gradients(states, action_for_gradients)\n",
    "\n",
    "        # todo finish this.\n",
    "\n",
    "    def _update_target_models(self):\n",
    "        \"\"\"\n",
    "        Updates the target models to slowly track the main models\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self._critic.train_target_model()\n",
    "        self._actor.train_target_model()\n",
    "\n",
    "    def remember(self, state, action, reward, done, next_state):\n",
    "        \"\"\"\n",
    "        Stores the given state, action, reward etc in the Agent's memory.\n",
    "        :param state: The state to remember\n",
    "        :param action: The action to remember\n",
    "        :param reward: The reward to remember\n",
    "        :param done: Whether this was a final state\n",
    "        :param next_state: The next state (if applicable)\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self._memory.append((state, action, reward, done, next_state))\n",
    "        if len(self._memory) > self._memory_size:\n",
    "            self._memory.popleft()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
